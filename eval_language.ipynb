{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0dd0910",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-23 16:24:22.493627: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/yongweic/.conda/envs/contrastive_rl/lib:/home/yongweic/.mujoco/mujoco210/bin:/usr/lib/nvidia:/home/yongweic/.mujoco/mujoco200/bin:/home/yongweic/.mujoco/mujoco200/bin\n",
      "Warning: Flow failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\n",
      "No module named 'flow'\n",
      "Warning: CARLA failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\n",
      "No module named 'carla'\n",
      "pybullet build time: Nov 28 2023 23:52:03\n",
      "/home/yongweic/.conda/envs/contrastive_rl/lib/python3.9/site-packages/gym/spaces/box.py:84: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import functools\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import animation\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from IPython.display import HTML\n",
    "\n",
    "import jax\n",
    "import optax\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from acme import specs\n",
    "from acme.tf.savers import SaveableAdapter\n",
    "\n",
    "import contrastive\n",
    "from contrastive.config import ContrastiveConfig\n",
    "from contrastive import utils as contrastive_utils\n",
    "from contrastive import make_networks\n",
    "from contrastive.utils import make_environment\n",
    "from contrastive import ContrastiveLearner\n",
    "from contrastive.builder import create_maze_dataset_iterator\n",
    "from tqdm import tqdm\n",
    "\n",
    "# disable tensorflow_probability warning: The use of `check_types` is deprecated and does not have any effect.\n",
    "import logging\n",
    "logger = logging.getLogger(\"root\")\n",
    "\n",
    "class CheckTypesFilter(logging.Filter):\n",
    "    def filter(self, record):\n",
    "        return \"check_types\" not in record.getMessage()\n",
    "\n",
    "logger.addFilter(CheckTypesFilter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1185957a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getConfig(env_name, ckpt_dir):\n",
    "    params = {\n",
    "      'use_random_actor': True,\n",
    "      'entropy_coefficient': None if 'image' in env_name else 0.0,\n",
    "      'env_name': env_name,\n",
    "      # For online RL experiments, max_number_of_steps is the number of\n",
    "      # environment steps. For offline RL experiments, this is the number of\n",
    "      # gradient steps.\n",
    "      'max_number_of_steps': 1_000_000,\n",
    "      'use_image_obs': 'image' in env_name,\n",
    "    }\n",
    "    if 'ant_' in env_name:\n",
    "        params['end_index'] = 2\n",
    "\n",
    "    # 2. Select an algorithm. The currently-supported algorithms are:\n",
    "    # contrastive_nce, contrastive_cpc, c_learning, nce+c_learning, gcbc.\n",
    "    # Many other algorithms can be implemented by passing other parameters\n",
    "    # or adding a few lines of code.\n",
    "    alg = 'contrastive_nce'\n",
    "    if alg == 'contrastive_nce':\n",
    "        pass  # Just use the default hyperparameters\n",
    "    elif alg == 'contrastive_cpc':\n",
    "        params['use_cpc'] = True\n",
    "    elif alg == 'c_learning':\n",
    "        params['use_td'] = True\n",
    "        params['twin_q'] = True\n",
    "    elif alg == 'nce+c_learning':\n",
    "        params['use_td'] = True\n",
    "        params['twin_q'] = True\n",
    "        params['add_mc_to_td'] = True\n",
    "    elif alg == 'gcbc':\n",
    "        params['use_gcbc'] = True\n",
    "    else:\n",
    "        raise NotImplementedError('Unknown method: %s' % alg)\n",
    "\n",
    "    # For the offline RL experiments, modify some hyperparameters.\n",
    "    if env_name.startswith('offline_ant'):\n",
    "        params.update({\n",
    "            # Effectively remove the rate-limiter by using very large values.\n",
    "            'samples_per_insert': 1_000_000,\n",
    "            'samples_per_insert_tolerance_rate': 100_000_000.0,\n",
    "            # For the actor update, only use future states as goals.\n",
    "            'random_goals': 0.0,\n",
    "            'bc_coef': 0.05,  # Add a behavioral cloning term to the actor.\n",
    "            'twin_q': True,  # Learn two critics, and take the minimum.\n",
    "            'batch_size': 1024,  # Increase the batch size 256 --> 1024.\n",
    "            'repr_dim': 16,  # Decrease the representation size 64 --> 16.\n",
    "            # Increase the policy network size (256, 256) --> (1024, 1024)\n",
    "            'hidden_layer_sizes': (1024, 1024),\n",
    "        })\n",
    "        print('hi')\n",
    "        \n",
    "    config = ContrastiveConfig(**params)\n",
    "    config.critic_learning_rate = 0.001\n",
    "    obs_dim = make_environment(env_name, config.start_index, config.end_index, seed=0)[1]\n",
    "\n",
    "    network_factory = functools.partial(\n",
    "      contrastive.make_networks, \n",
    "        obs_dim=obs_dim, \n",
    "        repr_dim=config.repr_dim,\n",
    "      repr_norm=config.repr_norm, \n",
    "        twin_q=config.twin_q,\n",
    "      use_image_obs=config.use_image_obs,\n",
    "      hidden_layer_sizes=config.hidden_layer_sizes)\n",
    "\n",
    "    env_factory = lambda seed: make_environment(\n",
    "      env_name, config.start_index, config.end_index, seed)[0]\n",
    "    dummy_seed = 1\n",
    "    environment_spec = specs.make_environment_spec(\n",
    "        env_factory(dummy_seed))\n",
    "    random_key = jax.random.PRNGKey(np.random.choice(int(1e6)))\n",
    "    networks = network_factory(environment_spec)\n",
    "    policy_optimizer = optax.adam(\n",
    "      learning_rate=config.actor_learning_rate)\n",
    "    q_optimizer = optax.adam(\n",
    "      learning_rate=config.critic_learning_rate)\n",
    "    l_optimizer = optax.adam(\n",
    "        learning_rate=config.critic_learning_rate\n",
    "    )\n",
    "\n",
    "    trained_learner = ContrastiveLearner(\n",
    "      networks=networks,\n",
    "      rng=random_key,\n",
    "      policy_optimizer=policy_optimizer,\n",
    "      q_optimizer=q_optimizer,\n",
    "      l_optimizer=l_optimizer,\n",
    "      iterator=None,\n",
    "      counter=None,\n",
    "      logger=None,\n",
    "      config=config,\n",
    "      obs_to_goal=None,\n",
    "      l_iterator=None\n",
    "    )\n",
    "\n",
    "    ckpt = tf.train.Checkpoint(learner=SaveableAdapter(trained_learner))\n",
    "    ckpt_mgr = tf.train.CheckpointManager(\n",
    "        ckpt, ckpt_dir, 1)\n",
    "    ckpt.restore(ckpt_mgr.latest_checkpoint)\n",
    "\n",
    "    trained_learner_state = trained_learner._state\n",
    "\n",
    "    print(\"Model loaded from: {}\".format(ckpt_dir))\n",
    "\n",
    "    return config, trained_learner, env_factory, networks, environment_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9d652c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_shape: (8, 8)\n",
      "Model loaded from: /home/yongweic/acme/52ec6ac2-4922-11ef-9af7-ec2a72227f18/checkpoints/learner\n"
     ]
    }
   ],
   "source": [
    "env_name = \"point_FourRooms\"\n",
    "ckpt_dir = \"/home/yongweic/acme/52ec6ac2-4922-11ef-9af7-ec2a72227f18/checkpoints/learner\"\n",
    "\n",
    "config, trained_learner, env_factory, networks, env_spec = getConfig(env_name, ckpt_dir)\n",
    "iterator = create_maze_dataset_iterator(config)\n",
    "trained_learner_state = trained_learner._state\n",
    "\n",
    "scaling_factor = config.scaling_factor\n",
    "maze_shape = config.maze_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb1c96a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4)\n",
      "cur: [[2.8639956 4.259392 ]], goal: [[9.692768 5.029451]]\n"
     ]
    }
   ],
   "source": [
    "# example usage\n",
    "env = env_factory(np.random.randint(1e6))\n",
    "\n",
    "obs = env.reset().observation[None,:]\n",
    "print(obs.shape)\n",
    "print(f'cur: {obs[:,:2]}, goal: {obs[:,-2:]}')\n",
    "\n",
    "q_params = trained_learner_state.q_params\n",
    "\n",
    "action=np.zeros((1,2))\n",
    "\n",
    "sa_repr, g_repr, (state, goal) = networks.repr_fn(q_params, obs=obs, action=action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d988e34d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31c764c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x14ef6691f8e0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample = next(iterator)\n",
    "\n",
    "g_repr, l_repr, outer = networks.l_network.apply(trained_learner_state.l_params, sample[1][:16,:], sample[0][:16,:])\n",
    "\n",
    "plt.imshow(np.exp(outer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f03541c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "def l_loss(l_params, \n",
    "          l_data):\n",
    "    language, goal = l_data\n",
    "    batch_size = language.shape[0]\n",
    "\n",
    "    I = jnp.eye(batch_size)  # pylint: disable=invalid-name\n",
    "    _, _, logits = networks.l_network.apply(l_params, goal, language)\n",
    "\n",
    "    def loss_fn(_logits):  # pylint: disable=invalid-name\n",
    "        if config.use_cpc:\n",
    "            return (optax.softmax_cross_entropy(logits=_logits, labels=I)\n",
    "                  + 0.01 * jax.nn.logsumexp(_logits, axis=1)**2)\n",
    "        else:\n",
    "            return optax.sigmoid_binary_cross_entropy(logits=_logits, labels=I)\n",
    "\n",
    "    loss = loss_fn(logits)\n",
    "    loss = jnp.mean(loss)\n",
    "\n",
    "    # correct = (jnp.argmax(logits, axis=1) == jnp.argmax(I, axis=1))\n",
    "    # logits_pos = jnp.sum(logits * I) / jnp.sum(I)\n",
    "    # logits_neg = jnp.sum(logits * (1 - I)) / jnp.sum(1 - I)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "921081c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: 0.0039595975540578365\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "loss = l_loss(trained_learner_state.l_params, next(iterator))\n",
    "\n",
    "print(f'LOSS: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a31483d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def grid_search_action(env, networks, q_params, obs, num_samples=5, category=None):\n",
    "    action_low, action_high = env.action_space.low, env.action_space.high\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    grid = [np.linspace(action_low[i], action_high[i], num_samples) for i in range(action_dim)]\n",
    "    action_grid = np.array(np.meshgrid(*grid)).T.reshape(-1, action_dim)\n",
    "    best_action = None\n",
    "    best_dot_product = -np.inf\n",
    "    \n",
    "    if category:\n",
    "        one_hot = np.zeros(maze_shape)\n",
    "        one_hot[category] = 1\n",
    "        one_hot = one_hot[None,:]\n",
    "        g_repr, _, _ = networks.l_network.apply(trained_learner_state.l_params, np.zeros((1, 2)), one_hot)\n",
    "    else:\n",
    "        _, g_repr, _ = networks.repr_fn(q_params, obs=obs, action=np.zeros((1, 2)))\n",
    "\n",
    "    \n",
    "    for action in action_grid:\n",
    "        # Compute representations\n",
    "        action = np.array(action, dtype=np.float32)\n",
    "        sa_repr, _, _ = networks.repr_fn(q_params, obs=obs, action=action[None, :])\n",
    "        \n",
    "        # Compute dot product\n",
    "        # print(f'here: {g_repr.shape}')\n",
    "        dot_product = np.dot(sa_repr.flatten(), g_repr.flatten())\n",
    "\n",
    "        if dot_product > best_dot_product:\n",
    "            best_dot_product = dot_product\n",
    "            best_action = action\n",
    "\n",
    "    # Step the environment with the best action\n",
    "    transition = env.step(best_action)\n",
    "\n",
    "    return transition\n",
    "\n",
    "def run_episode(env, networks, q_params, n_steps=1, num_samples=1, categorical=False):\n",
    "    obs = env.reset().observation[None,:]\n",
    "    \n",
    "    category=None\n",
    "    if categorical:\n",
    "        category = np.random.randint(0, 16)\n",
    "    \n",
    "    \n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    for step in tqdm(range(n_steps)):\n",
    "#         print(f\"\\nStep {step + 1}/{n_steps}\")\n",
    "#         print(f\"Current observation: {obs[:,:2]}, Goal: {obs[:,-2:]}\")\n",
    "        \n",
    "        transition = grid_search_action(env, networks, q_params, obs, num_samples, category=category)\n",
    "        \n",
    "        obs = transition.observation[None,:]\n",
    "        # print(obs[:,:2])\n",
    "        # points.append(obs[0,:2])\n",
    "        \n",
    "        cur = obs[0,:2]\n",
    "        goal = obs[0,-2:]\n",
    "        \n",
    "        \n",
    "        if categorical:\n",
    "            # print(f'category: {category} and position: {cur}')\n",
    "            cond1 = category < 8 and category == int(cur[0])\n",
    "            cond2 = category >= 8 and category == int(cur[1]) - 8\n",
    "            if cond1 or cond2:\n",
    "                done = True\n",
    "                break\n",
    "        else:\n",
    "            if np.linalg.norm(cur - goal) < 0.5:\n",
    "                done= True\n",
    "                break\n",
    "        \n",
    "    \n",
    "    return done\n",
    "\n",
    "# Example usage:\n",
    "NUM_TRIALS = 5\n",
    "success_rate = 0\n",
    "for i in range(NUM_TRIALS):\n",
    "    done = run_episode(env, networks, q_params, n_steps=25, num_samples=10, categorical=True)\n",
    "    success_rate += done\n",
    "print(f'success: {success_rate / NUM_TRIALS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84830d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.array(points)\n",
    "plt.scatter(p[:,0], p[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221b30f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bfa5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_repr, g_repr, (state, goal) = networks.repr_fn(q_params, obs=obs, action=action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea77d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPISODES = 25\n",
    "\n",
    "env = env_factory(np.random.randint(1e6))\n",
    "obs_dim = env.observation_spec().shape[0] // 2\n",
    "episode_returns = np.zeros([NUM_EPISODES, ])\n",
    "\n",
    "timesteps = []\n",
    "for epi in range(NUM_EPISODES):\n",
    "    t = 0\n",
    "    env.seed(epi)  # use fixed seed for different methods\n",
    "    timestep = env.reset()\n",
    "    episode_return = 0\n",
    "    \n",
    "    # print(f'start state: {timestep.observation[:2]}, goal: {timestep.observation[-2:]}')\n",
    "    # goal = [5.9277008955091254, 8.699144684746049]\n",
    "\n",
    "    while not timestep.last():\n",
    "        obs = timestep.observation\n",
    "        # print(obs.shape)\n",
    "        # obs[-2:] = goal\n",
    "        dist = networks.policy_network.apply(\n",
    "          trained_learner_state.policy_params,\n",
    "          timestep.observation\n",
    "        )\n",
    "        action = np.array(dist.mode())\n",
    "        # print(action)\n",
    "        timestep = env.step(action)\n",
    "        timesteps.append(timestep)\n",
    "\n",
    "        # Book-keeping.\n",
    "        t += 1\n",
    "        episode_return += timestep.reward\n",
    "\n",
    "    # assert t == env._step_limit\n",
    "    # print(\"episode length = {}\".format(t))\n",
    "    episode_returns[epi] = episode_return\n",
    "\n",
    "print(\"avg episode return: {}\".format(np.mean(episode_returns)))\n",
    "print(\"success rate: {}\".format(np.mean(episode_returns >= 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaead1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95586d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = []\n",
    "for timestep in timesteps:\n",
    "    points.append(timestep.observation[:2])\n",
    "    \n",
    "points = np.array(points)\n",
    "plt.scatter(points[:,0], points[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad358d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cf7358",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "contrastive_rl [~/.conda/envs/contrastive_rl/]",
   "language": "python",
   "name": "conda_contrastive_rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
