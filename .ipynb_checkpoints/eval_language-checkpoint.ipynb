{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dd0910",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import functools\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import animation\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from IPython.display import HTML\n",
    "\n",
    "import jax\n",
    "import optax\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from acme import specs\n",
    "from acme.tf.savers import SaveableAdapter\n",
    "\n",
    "import contrastive\n",
    "from contrastive.config import ContrastiveConfig\n",
    "from contrastive import utils as contrastive_utils\n",
    "from contrastive import make_networks\n",
    "from contrastive.utils import make_environment\n",
    "from contrastive import ContrastiveLearner\n",
    "from contrastive.builder import create_maze_dataset_iterator\n",
    "from tqdm import tqdm\n",
    "\n",
    "# disable tensorflow_probability warning: The use of `check_types` is deprecated and does not have any effect.\n",
    "import logging\n",
    "logger = logging.getLogger(\"root\")\n",
    "\n",
    "class CheckTypesFilter(logging.Filter):\n",
    "    def filter(self, record):\n",
    "        return \"check_types\" not in record.getMessage()\n",
    "\n",
    "logger.addFilter(CheckTypesFilter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1185957a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getConfig(env_name, ckpt_dir):\n",
    "    params = {\n",
    "      'use_random_actor': True,\n",
    "      'entropy_coefficient': None if 'image' in env_name else 0.0,\n",
    "      'env_name': env_name,\n",
    "      # For online RL experiments, max_number_of_steps is the number of\n",
    "      # environment steps. For offline RL experiments, this is the number of\n",
    "      # gradient steps.\n",
    "      'max_number_of_steps': 1_000_000,\n",
    "      'use_image_obs': 'image' in env_name,\n",
    "    }\n",
    "    if 'ant_' in env_name:\n",
    "        params['end_index'] = 2\n",
    "\n",
    "    # 2. Select an algorithm. The currently-supported algorithms are:\n",
    "    # contrastive_nce, contrastive_cpc, c_learning, nce+c_learning, gcbc.\n",
    "    # Many other algorithms can be implemented by passing other parameters\n",
    "    # or adding a few lines of code.\n",
    "    alg = 'contrastive_nce'\n",
    "    if alg == 'contrastive_nce':\n",
    "        pass  # Just use the default hyperparameters\n",
    "    elif alg == 'contrastive_cpc':\n",
    "        params['use_cpc'] = True\n",
    "    elif alg == 'c_learning':\n",
    "        params['use_td'] = True\n",
    "        params['twin_q'] = True\n",
    "    elif alg == 'nce+c_learning':\n",
    "        params['use_td'] = True\n",
    "        params['twin_q'] = True\n",
    "        params['add_mc_to_td'] = True\n",
    "    elif alg == 'gcbc':\n",
    "        params['use_gcbc'] = True\n",
    "    else:\n",
    "        raise NotImplementedError('Unknown method: %s' % alg)\n",
    "\n",
    "    # For the offline RL experiments, modify some hyperparameters.\n",
    "    if env_name.startswith('offline_ant'):\n",
    "        params.update({\n",
    "            # Effectively remove the rate-limiter by using very large values.\n",
    "            'samples_per_insert': 1_000_000,\n",
    "            'samples_per_insert_tolerance_rate': 100_000_000.0,\n",
    "            # For the actor update, only use future states as goals.\n",
    "            'random_goals': 0.0,\n",
    "            'bc_coef': 0.05,  # Add a behavioral cloning term to the actor.\n",
    "            'twin_q': True,  # Learn two critics, and take the minimum.\n",
    "            'batch_size': 1024,  # Increase the batch size 256 --> 1024.\n",
    "            'repr_dim': 16,  # Decrease the representation size 64 --> 16.\n",
    "            # Increase the policy network size (256, 256) --> (1024, 1024)\n",
    "            'hidden_layer_sizes': (1024, 1024),\n",
    "        })\n",
    "        print('hi')\n",
    "        \n",
    "    config = ContrastiveConfig(**params)\n",
    "    config.critic_learning_rate = 0.001\n",
    "    obs_dim = make_environment(env_name, config.start_index, config.end_index, seed=0)[1]\n",
    "\n",
    "    network_factory = functools.partial(\n",
    "      contrastive.make_networks, \n",
    "        obs_dim=obs_dim, \n",
    "        repr_dim=config.repr_dim,\n",
    "      repr_norm=config.repr_norm, \n",
    "        twin_q=config.twin_q,\n",
    "      use_image_obs=config.use_image_obs,\n",
    "      hidden_layer_sizes=config.hidden_layer_sizes)\n",
    "\n",
    "    env_factory = lambda seed: make_environment(\n",
    "      env_name, config.start_index, config.end_index, seed)[0]\n",
    "    dummy_seed = 1\n",
    "    environment_spec = specs.make_environment_spec(\n",
    "        env_factory(dummy_seed))\n",
    "    random_key = jax.random.PRNGKey(np.random.choice(int(1e6)))\n",
    "    networks = network_factory(environment_spec)\n",
    "    policy_optimizer = optax.adam(\n",
    "      learning_rate=config.actor_learning_rate)\n",
    "    q_optimizer = optax.adam(\n",
    "      learning_rate=config.critic_learning_rate)\n",
    "    l_optimizer = optax.adam(\n",
    "        learning_rate=config.critic_learning_rate\n",
    "    )\n",
    "\n",
    "    trained_learner = ContrastiveLearner(\n",
    "      networks=networks,\n",
    "      rng=random_key,\n",
    "      policy_optimizer=policy_optimizer,\n",
    "      q_optimizer=q_optimizer,\n",
    "      l_optimizer=l_optimizer,\n",
    "      iterator=None,\n",
    "      counter=None,\n",
    "      logger=None,\n",
    "      config=config,\n",
    "      obs_to_goal=None,\n",
    "      l_iterator=None\n",
    "    )\n",
    "\n",
    "    ckpt = tf.train.Checkpoint(learner=SaveableAdapter(trained_learner))\n",
    "    ckpt_mgr = tf.train.CheckpointManager(\n",
    "        ckpt, ckpt_dir, 1)\n",
    "    ckpt.restore(ckpt_mgr.latest_checkpoint)\n",
    "\n",
    "    trained_learner_state = trained_learner._state\n",
    "\n",
    "    print(\"Model loaded from: {}\".format(ckpt_dir))\n",
    "\n",
    "    return config, trained_learner, env_factory, networks, environment_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a1e1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_spec.action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d652c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"ant_umaze\"\n",
    "ckpt_dir = \"/home/yongweic/acme/a336176c-4918-11ef-9d3c-ec2a7222924e/checkpoints/counter\"\n",
    "\n",
    "config, trained_learner, env_factory, networks, env_spec = getConfig(env_name, ckpt_dir)\n",
    "iterator = create_maze_dataset_iterator((8, 8), config)\n",
    "trained_learner_state = trained_learner._state\n",
    "\n",
    "scaling_factor = config.scaling_factor\n",
    "maze_shape = config.maze_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07d5764",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf4178d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.scaling_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1c96a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example usage\n",
    "env = env_factory(np.random.randint(1e6))\n",
    "\n",
    "obs = env.reset().observation[None,:]\n",
    "print(obs.shape)\n",
    "print(f'cur: {obs[:,:2]}, goal: {obs[:,-2:]}')\n",
    "\n",
    "q_params = trained_learner_state.q_params\n",
    "\n",
    "action=np.zeros((1,2))\n",
    "\n",
    "sa_repr, g_repr, (state, goal) = networks.repr_fn(q_params, obs=obs, action=action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b893b9b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c764c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iterator)\n",
    "\n",
    "g_repr, l_repr, outer = networks.l_network.apply(trained_learner_state.l_params, sample[1][:16,:], sample[0][:16,:])\n",
    "\n",
    "plt.imshow(np.exp(outer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03541c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "def l_loss(l_params, \n",
    "          l_data):\n",
    "    language, goal = l_data\n",
    "    batch_size = language.shape[0]\n",
    "\n",
    "    I = jnp.eye(batch_size)  # pylint: disable=invalid-name\n",
    "    _, _, logits = networks.l_network.apply(l_params, goal, language)\n",
    "\n",
    "    def loss_fn(_logits):  # pylint: disable=invalid-name\n",
    "        if config.use_cpc:\n",
    "            return (optax.softmax_cross_entropy(logits=_logits, labels=I)\n",
    "                  + 0.01 * jax.nn.logsumexp(_logits, axis=1)**2)\n",
    "        else:\n",
    "            return optax.sigmoid_binary_cross_entropy(logits=_logits, labels=I)\n",
    "\n",
    "    loss = loss_fn(logits)\n",
    "    loss = jnp.mean(loss)\n",
    "\n",
    "    # correct = (jnp.argmax(logits, axis=1) == jnp.argmax(I, axis=1))\n",
    "    # logits_pos = jnp.sum(logits * I) / jnp.sum(I)\n",
    "    # logits_neg = jnp.sum(logits * (1 - I)) / jnp.sum(1 - I)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921081c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "loss = l_loss(trained_learner_state.l_params, next(iterator))\n",
    "\n",
    "print(f'LOSS: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a31483d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def grid_search_action(env, networks, q_params, obs, num_samples=5, category=None):\n",
    "    action_low, action_high = env.action_space.low, env.action_space.high\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    grid = [np.linspace(action_low[i], action_high[i], num_samples) for i in range(action_dim)]\n",
    "    action_grid = np.array(np.meshgrid(*grid)).T.reshape(-1, action_dim)\n",
    "    best_action = None\n",
    "    best_dot_product = -np.inf\n",
    "    \n",
    "    if category:\n",
    "        one_hot = np.zeros(16)\n",
    "        one_hot[category] = 1\n",
    "        one_hot = one_hot[None,:]\n",
    "        g_repr, _, _ = networks.l_network.apply(trained_learner_state.l_params, np.zeros((1, 2)), one_hot)\n",
    "    else:\n",
    "        _, g_repr, _ = networks.repr_fn(q_params, obs=obs, action=np.zeros((1, 2)))\n",
    "\n",
    "    \n",
    "    for action in action_grid:\n",
    "        # Compute representations\n",
    "        action = np.array(action, dtype=np.float32)\n",
    "        sa_repr, _, _ = networks.repr_fn(q_params, obs=obs, action=action[None, :])\n",
    "        \n",
    "        # Compute dot product\n",
    "        # print(f'here: {g_repr.shape}')\n",
    "        dot_product = np.dot(sa_repr.flatten(), g_repr.flatten())\n",
    "\n",
    "        if dot_product > best_dot_product:\n",
    "            best_dot_product = dot_product\n",
    "            best_action = action\n",
    "\n",
    "    # Step the environment with the best action\n",
    "    transition = env.step(best_action)\n",
    "\n",
    "    return transition\n",
    "\n",
    "def run_episode(env, networks, q_params, n_steps=1, num_samples=1, categorical=False):\n",
    "    obs = env.reset().observation[None,:]\n",
    "    \n",
    "    category=None\n",
    "    if categorical:\n",
    "        category = np.random.randint(0, 16)\n",
    "    \n",
    "    \n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    for step in tqdm(range(n_steps)):\n",
    "#         print(f\"\\nStep {step + 1}/{n_steps}\")\n",
    "#         print(f\"Current observation: {obs[:,:2]}, Goal: {obs[:,-2:]}\")\n",
    "        \n",
    "        transition = grid_search_action(env, networks, q_params, obs, num_samples, category=category)\n",
    "        \n",
    "        obs = transition.observation[None,:]\n",
    "        # print(obs[:,:2])\n",
    "        # points.append(obs[0,:2])\n",
    "        \n",
    "        cur = obs[0,:2]\n",
    "        goal = obs[0,-2:]\n",
    "        \n",
    "        \n",
    "        if categorical:\n",
    "            # print(f'category: {category} and position: {cur}')\n",
    "            cond1 = category < 8 and category == int(cur[0])\n",
    "            cond2 = category >= 8 and category == int(cur[1]) - 8\n",
    "            if cond1 or cond2:\n",
    "                done = True\n",
    "                break\n",
    "        else:\n",
    "            if np.linalg.norm(cur - goal) < 0.5:\n",
    "                done= True\n",
    "                break\n",
    "        \n",
    "    \n",
    "    return done\n",
    "\n",
    "# Example usage:\n",
    "NUM_TRIALS = 5\n",
    "success_rate = 0\n",
    "for i in range(NUM_TRIALS):\n",
    "    done = run_episode(env, networks, q_params, n_steps=25, num_samples=10, categorical=True)\n",
    "    success_rate += done\n",
    "print(f'success: {success_rate / NUM_TRIALS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84830d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.array(points)\n",
    "plt.scatter(p[:,0], p[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221b30f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bfa5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_repr, g_repr, (state, goal) = networks.repr_fn(q_params, obs=obs, action=action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea77d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPISODES = 25\n",
    "\n",
    "env = env_factory(np.random.randint(1e6))\n",
    "obs_dim = env.observation_spec().shape[0] // 2\n",
    "episode_returns = np.zeros([NUM_EPISODES, ])\n",
    "\n",
    "timesteps = []\n",
    "for epi in range(NUM_EPISODES):\n",
    "    t = 0\n",
    "    env.seed(epi)  # use fixed seed for different methods\n",
    "    timestep = env.reset()\n",
    "    episode_return = 0\n",
    "    \n",
    "    # print(f'start state: {timestep.observation[:2]}, goal: {timestep.observation[-2:]}')\n",
    "    # goal = [5.9277008955091254, 8.699144684746049]\n",
    "\n",
    "    while not timestep.last():\n",
    "        obs = timestep.observation\n",
    "        # print(obs.shape)\n",
    "        # obs[-2:] = goal\n",
    "        dist = networks.policy_network.apply(\n",
    "          trained_learner_state.policy_params,\n",
    "          timestep.observation\n",
    "        )\n",
    "        action = np.array(dist.mode())\n",
    "        # print(action)\n",
    "        timestep = env.step(action)\n",
    "        timesteps.append(timestep)\n",
    "\n",
    "        # Book-keeping.\n",
    "        t += 1\n",
    "        episode_return += timestep.reward\n",
    "\n",
    "    # assert t == env._step_limit\n",
    "    # print(\"episode length = {}\".format(t))\n",
    "    episode_returns[epi] = episode_return\n",
    "\n",
    "print(\"avg episode return: {}\".format(np.mean(episode_returns)))\n",
    "print(\"success rate: {}\".format(np.mean(episode_returns >= 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaead1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95586d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = []\n",
    "for timestep in timesteps:\n",
    "    points.append(timestep.observation[:2])\n",
    "    \n",
    "points = np.array(points)\n",
    "plt.scatter(points[:,0], points[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad358d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cf7358",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "contrastive_rl [~/.conda/envs/contrastive_rl/]",
   "language": "python",
   "name": "conda_contrastive_rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
